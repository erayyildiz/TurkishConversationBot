{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeylaMecnunChatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erayyildiz/TurkishConversationBot/blob/master/TurkishConversation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6PoyurBM60q1",
        "colab_type": "code",
        "outputId": "da0dd7f1-a668-460b-a792-9e9aa53402ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch --upgrade\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     \\ 512kB 572kB/s\n",
            "Building wheels for collected packages: tqdm\n",
            "  Running setup.py bdist_wheel for tqdm ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-ea73c91c/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yZVcQ-ZTBO9v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os.path\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcBA37HZZwS6",
        "colab_type": "code",
        "outputId": "986ba313-26fe-4806-d2fd-e8e2084ca327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EZ7ooIgCiu9F",
        "outputId": "90339593-b763-41e5-c1c5-23a769ec4907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pw70uZAypGa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "create_subtitle_utterences=False\n",
        "if create_subtitle_utterences:\n",
        "    !wget http://opus.nlpl.eu/download.php?f=OpenSubtitles2018%2Fmono%2FOpenSubtitles2018.raw.tr.gz\n",
        "    !gunzip 'download.php?f=OpenSubtitles2018%2Fmono%2FOpenSubtitles2018.raw.tr.gz'\n",
        "    !mv 'download.php?f=OpenSubtitles2018%2Fmono%2FOpenSubtitles2018.raw.tr' subtitles.txt\n",
        "    previous_line = \"\"\n",
        "    with open(\"subtitles.dialogues.txt\", \"w\") as w:\n",
        "        with open(\"subtitles.txt\", \"r\") as f:\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.strip()\n",
        "                if line.startswith(\"-\"):\n",
        "                    line = line.replace(\"- \", \"\")\n",
        "                    line = line.replace(\"-\", \"\")\n",
        "                    if previous_line:\n",
        "                        w.write(\"{}\\t{}\\n\".format(previous_line, line))\n",
        "                    previous_line = line\n",
        "                else:\n",
        "                    previous_line = \"\"\n",
        "    !mkdir \"gdrive/My Drive/chatbot\"\n",
        "    !cp subtitles.dialogues.txt \"gdrive/My Drive/chatbot/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Xyr8sro-zPz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "from nltk.tokenize import casual_tokenize\n",
        "\n",
        "EOS = \"<EOS>\"\n",
        "SOS = \"<SOS>\"\n",
        "PAD = \"<PAD>\"\n",
        "MAX_LEN = 25\n",
        "\n",
        "def to_lower(txt):\n",
        "    txt = txt.replace('İ', 'i')\n",
        "    txt = txt.replace('Ğ', 'ğ')\n",
        "    txt = txt.replace('Ü', 'ü')\n",
        "    txt = txt.replace('Ş', 'ş')\n",
        "    txt = txt.replace('Ç', 'ç')\n",
        "    txt = txt.replace('Ö', 'ö')\n",
        "    return txt.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wp6nOnm-pyCO",
        "colab_type": "code",
        "outputId": "b8241bb0-396f-462d-e770-55670d10f578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "cell_type": "code",
      "source": [
        "utterance_regex = re.compile(r\"^.*[a-zA-Z0-9]+.*$\")\n",
        "\n",
        "subtitle_utterances = []\n",
        "\n",
        "def add_dialogue(q, a, subtitle_utterances):\n",
        "    q = to_lower(q)\n",
        "    a = to_lower(a).strip()\n",
        "    q_tokens = [token for token in casual_tokenize(q) if utterance_regex.match(token)]\n",
        "    a_tokens = [token for token in casual_tokenize(a) if utterance_regex.match(token)]\n",
        "    \n",
        "    if (MAX_LEN - 2) > len(q_tokens) > 0 and (MAX_LEN - 3) > len(a_tokens) > 0:\n",
        "        subtitle_utterances.append((q_tokens, a_tokens))\n",
        "        added_utterances = len(subtitle_utterances)\n",
        "        if added_utterances % 100000 == 0:\n",
        "            print(\"{} dialogues are read so far.\".format(added_utterances))\n",
        "\n",
        "\n",
        "with open(\"gdrive/My Drive/chatbot/subtitles.dialogues.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        parses = line.split(\"\\t\")\n",
        "        q = parses[0].strip()\n",
        "        # if q.endswith(\"?\") or any([question_word in q for question_word in question_words]):\n",
        "        add_dialogue(q, parses[1], subtitle_utterances) \n",
        "        if len(subtitle_utterances) > 7000000:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 dialogues are read so far.\n",
            "200000 dialogues are read so far.\n",
            "300000 dialogues are read so far.\n",
            "400000 dialogues are read so far.\n",
            "500000 dialogues are read so far.\n",
            "600000 dialogues are read so far.\n",
            "700000 dialogues are read so far.\n",
            "800000 dialogues are read so far.\n",
            "900000 dialogues are read so far.\n",
            "1000000 dialogues are read so far.\n",
            "1100000 dialogues are read so far.\n",
            "1200000 dialogues are read so far.\n",
            "1300000 dialogues are read so far.\n",
            "1400000 dialogues are read so far.\n",
            "1500000 dialogues are read so far.\n",
            "1600000 dialogues are read so far.\n",
            "1700000 dialogues are read so far.\n",
            "1800000 dialogues are read so far.\n",
            "1900000 dialogues are read so far.\n",
            "2000000 dialogues are read so far.\n",
            "2100000 dialogues are read so far.\n",
            "2200000 dialogues are read so far.\n",
            "2300000 dialogues are read so far.\n",
            "2400000 dialogues are read so far.\n",
            "2500000 dialogues are read so far.\n",
            "2600000 dialogues are read so far.\n",
            "2700000 dialogues are read so far.\n",
            "2800000 dialogues are read so far.\n",
            "2900000 dialogues are read so far.\n",
            "3000000 dialogues are read so far.\n",
            "3100000 dialogues are read so far.\n",
            "3200000 dialogues are read so far.\n",
            "3300000 dialogues are read so far.\n",
            "3400000 dialogues are read so far.\n",
            "3500000 dialogues are read so far.\n",
            "3600000 dialogues are read so far.\n",
            "3700000 dialogues are read so far.\n",
            "3800000 dialogues are read so far.\n",
            "3900000 dialogues are read so far.\n",
            "4000000 dialogues are read so far.\n",
            "4100000 dialogues are read so far.\n",
            "4200000 dialogues are read so far.\n",
            "4300000 dialogues are read so far.\n",
            "4400000 dialogues are read so far.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XJOMjTohJZq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_counts_dic = {}\n",
        "for utterance in subtitle_utterances:\n",
        "    for word in utterance[0]:\n",
        "        if word not in word_counts_dic:\n",
        "            word_counts_dic[word] = 1\n",
        "        else:\n",
        "            word_counts_dic[word] += 1\n",
        "\n",
        "min_word_count = 10\n",
        "words = [PAD, SOS, EOS]\n",
        "words += [w for w, c in word_counts_dic.items() if c >= min_word_count]\n",
        "\n",
        "words = list(words)\n",
        "int2word = words\n",
        "word2int = {c:i for i,c in enumerate(words)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hafh1Z24BVnX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(sentence):\n",
        "    return [word2int[word] for word in sentence if word in word2int]\n",
        "\n",
        "\n",
        "def tensorFromSentence(sentence, max_len=MAX_LEN, add_start_tag=False):\n",
        "    indexes = indexesFromSentence(sentence)\n",
        "    if add_start_tag:\n",
        "        indexes = [word2int[SOS]] + indexes \n",
        "    indexes.append(word2int[EOS])\n",
        "    for i in range(len(indexes), max_len):\n",
        "        indexes.append(word2int[PAD])\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(pair[0])\n",
        "    target_tensor = tensorFromSentence(pair[1], add_start_tag=True)\n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXGtoTaV_1Rj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class UtteranceDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, utterances, words):\n",
        "        self.utterances = utterances\n",
        "        self.int2word = words\n",
        "        self.word2int = {c:i for i,c in enumerate(words)}\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.utterances)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        utterance = self.utterances[index]\n",
        "        return tensorsFromPair(utterance)\n",
        "    \n",
        "utterance_dataset = UtteranceDataset(subtitle_utterances, words)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vVoT2WUVB2Fy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(utterance_dataset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_uGW8pp0BXyM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ENCODER_PATH = \"gdrive/My Drive/chatbot/encoder.model\"\n",
        "DECODER_PATH = \"gdrive/My Drive/chatbot/decoder.model\"        \n",
        "        \n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output = embedded\n",
        "        hidden = self.init_hidden(x.size(0))\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "    \n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, hidden, y):\n",
        "        output = self.embedding(y)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output\n",
        "    \n",
        "    def predict(self, hidden, max_len=25):\n",
        "        decoded_words = []\n",
        "        decoder_input = torch.tensor([[word2int[SOS]]], device=device)\n",
        "        input_embedding = self.embedding(decoder_input)\n",
        "        for i in range(max_len):\n",
        "            output, hidden = self.gru(input_embedding, hidden)\n",
        "            output = self.softmax(self.out(output[0]))\n",
        "            topv, topi = output.data.topk(1)\n",
        "            if topi.item() == word2int[EOS]:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(int2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            decoder_input = torch.tensor([[decoder_input]], device=device)\n",
        "            input_embedding = self.embedding(decoder_input)\n",
        "\n",
        "        return decoded_words\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRc_ncQpBa6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    loss = 0\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    decoder_outputs = decoder(encoder_hidden, target_tensor)\n",
        "    decoder_outputs = decoder_outputs[:, :-1, :]\n",
        "    decoder_outputs = decoder_outputs.contiguous().view(decoder_outputs.shape[0]*decoder_outputs.shape[1], -1)\n",
        "    expected_outputs = target_tensor[:, 1:].contiguous().view(1, -1).squeeze()\n",
        "    \n",
        "    loss = criterion(decoder_outputs, expected_outputs)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2OblheyCzn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 512\n",
        "encoder = EncoderRNN(len(words), hidden_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size, len(words)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7a8Rex57Cvn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence):\n",
        "    with torch.no_grad():\n",
        "        if isinstance(sentence, str):\n",
        "            tokens = casual_tokenize(to_lower(sentence))\n",
        "        elif isinstance(sentence, list):\n",
        "            tokens = sentence\n",
        "        else:\n",
        "            raise TypeError(\"Wrong Input Type: {}\".format(type(sentence)))\n",
        "        input_tensor = tensorFromSentence(tokens)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        input_tensor = input_tensor.view(1, *input_tensor.size())\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        predictions = decoder.predict(encoder_hidden)\n",
        "        return ' '.join(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z8RJyyhMCxWo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, utterances, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(utterances)\n",
        "        output_sentence = evaluate(encoder, decoder, pair[0])\n",
        "        yield '{} <=> {}'.format(' '.join(pair[0]), output_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sHASAdgti2_r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_model_states(model, path):\n",
        "    \"\"\"Load a previously saved model states.\"\"\"\n",
        "    # original saved file with DataParallel\n",
        "    state_dict = torch.load(path)\n",
        "    model.load_state_dict(state_dict)\n",
        "        \n",
        "if os.path.isfile(ENCODER_PATH):\n",
        "    print(\"Loading Encoder\")\n",
        "    load_model_states(encoder, ENCODER_PATH)\n",
        "if os.path.isfile(DECODER_PATH):\n",
        "    print(\"Loading Decoder\")\n",
        "    load_model_states(decoder, DECODER_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6-H2-ijCqjA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "encoder_optimizer = optim.Adam(encoder.parameters())\n",
        "decoder_optimizer = optim.Adam(decoder.parameters())\n",
        "\n",
        "\n",
        "print_loss_total = 0  \n",
        "\n",
        "print_every = 500\n",
        "save_every = 5000\n",
        "\n",
        "encoder_optimizer.zero_grad()\n",
        "decoder_optimizer.zero_grad()\n",
        "loss = 0.0\n",
        "\n",
        "for i, (input_tensor, target_tensor) in enumerate(tqdm(data_loader, desc='Training:')):\n",
        "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "\n",
        "    print_loss_total += loss.item()\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    if i % print_every == 0:\n",
        "        print_loss_avg = print_loss_total / print_every\n",
        "        print_loss_total = 0\n",
        "        print(' Loss: {}'.format(print_loss_avg))\n",
        "    if i % save_every == 0:\n",
        "        for message in evaluateRandomly(encoder, decoder, subtitle_utterances):\n",
        "            print(message)\n",
        "        torch.save(encoder.state_dict(), ENCODER_PATH)\n",
        "        torch.save(decoder.state_dict(), DECODER_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCzjK-J6FW8o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "\n",
        "while True:\n",
        "    clear_output()\n",
        "    time.sleep(1)\n",
        "    input_txt = input()\n",
        "    output_words = evaluate(encoder, decoder, input_txt)\n",
        "    output_sentence = ' '.join(output_words[:-1])\n",
        "    print(\" - {}\".format(output_sentence))\n",
        "    time.sleep(2)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}