{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeylaMecnunChatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/erayyildiz/TurkishConversationBot/blob/master/LeylaMecnunChatbot.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "6PoyurBM60q1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "c2e8a9aa-05cb-41f3-95c0-f711b066bb97"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch --upgrade "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 29kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x59d14000 @  0x7f13b27ea2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yZVcQ-ZTBO9v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcBA37HZZwS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "910156eb-f1c0-4269-89c7-359e990eddc8"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "bb1bd608-4ee0-4881-957b-cd3f8b719f6c",
        "id": "EZ7ooIgCiu9F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pw70uZAypGa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "create_subtitle_utterences=False\n",
        "if create_subtitle_utterences:\n",
        "    !wget http://opus.nlpl.eu/download.php?f=OpenSubtitles2018%2Fmono%2FOpenSubtitles2018.raw.tr.gz\n",
        "    !gunzip 'download.php?f=OpenSubtitles2018%2Fmono%2FOpenSubtitles2018.raw.tr.gz'\n",
        "    !mv 'download.php?f=OpenSubtitles2018%2Fmono%2FOpenSubtitles2018.raw.tr' subtitles.txt\n",
        "    previous_line = \"\"\n",
        "    with open(\"subtitles.dialogues.txt\", \"w\") as w:\n",
        "        with open(\"subtitles.txt\", \"r\") as f:\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.strip()\n",
        "                if line.startswith(\"-\"):\n",
        "                    line = line.replace(\"- \", \"\")\n",
        "                    line = line.replace(\"-\", \"\")\n",
        "                    if previous_line:\n",
        "                        w.write(\"{}\\t{}\\n\".format(previous_line, line))\n",
        "                    previous_line = line\n",
        "                else:\n",
        "                    previous_line = \"\"\n",
        "    !mkdir \"gdrive/My Drive/chatbot\"\n",
        "    !cp subtitles.dialogues.txt \"gdrive/My Drive/chatbot/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Xyr8sro-zPz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from nltk.tokenize import casual_tokenize\n",
        "\n",
        "EOS = \"<EOS>\"\n",
        "SOS = \"<SOS>\"\n",
        "\n",
        "def to_lower(txt):\n",
        "    txt = txt.replace('İ', 'i')\n",
        "    txt = txt.replace('Ğ', 'ğ')\n",
        "    txt = txt.replace('Ü', 'ü')\n",
        "    txt = txt.replace('Ş', 'ş')\n",
        "    txt = txt.replace('Ç', 'ç')\n",
        "    txt = txt.replace('Ö', 'ö')\n",
        "    return txt.lower()\n",
        "    \n",
        "\n",
        "utterance_regex = re.compile(r\"^.*[a-zA-Z]+.*$\")\n",
        "nondiaolgue_regex = re.compile(r\"\\[[^\\]]+\\]\")\n",
        "nondiaolgue_regex2 = re.compile(r\"<[^>]+>\")\n",
        "number_regex = re.compile(r\"^[0-9]+$\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wp6nOnm-pyCO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_samples = 5000000 \n",
        "subtitle_utterances = []\n",
        "with open(\"gdrive/My Drive/chatbot/subtitles.dialogues.txt\", \"r\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        parses = line.split(\"\\t\")\n",
        "        q = to_lower(parses[0])\n",
        "        a = to_lower(parses[1])\n",
        "        q_tokens = [token for token in casual_tokenize(q) if utterance_regex.match(token)]\n",
        "        a_tokens = [token for token in casual_tokenize(a) if utterance_regex.match(token)]\n",
        "        subtitle_utterances.append((q_tokens, a_tokens))\n",
        "        if i >= max_samples:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ERSZrOvkJPJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "462aff28-939c-48bf-807d-a3238eae7a0b"
      },
      "cell_type": "code",
      "source": [
        "len(subtitle_utterances)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "hafh1Z24BVnX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(sentence):\n",
        "    return [word2int[word] for word in sentence if word in word2int]\n",
        "\n",
        "\n",
        "def tensorFromSentence(sentence):\n",
        "    indexes = indexesFromSentence(sentence)\n",
        "    indexes.append(word2int[EOS])\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(pair[0])\n",
        "    target_tensor = tensorFromSentence(pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_uGW8pp0BXyM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        \n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        \n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "    \n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPSdIVFgVpW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "87e0f15c-4be3-407d-aeba-67c6b51074b2"
      },
      "cell_type": "code",
      "source": [
        "device"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "uRc_ncQpBa6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=50):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[word2int[SOS]]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True\n",
        "    # if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == word2int[EOS]:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coUheE13Cog5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6-H2-ijCqjA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, utterances, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(utterances))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7a8Rex57Cvn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=20, beam_size=10):\n",
        "    with torch.no_grad():\n",
        "        if isinstance(sentence, str):\n",
        "            tokens = casual_tokenize(to_lower(sentence))\n",
        "        elif isinstance(sentence, list):\n",
        "            tokens = sentence\n",
        "        else:\n",
        "            raise TypeError(\"Wrong Input Type: {}\".format(type(sentence)))\n",
        "        input_tensor = tensorFromSentence(tokens)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[word2int[SOS]]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == word2int[EOS]:\n",
        "                decoded_words.append(EOS)\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(int2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z8RJyyhMCxWo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, utterances, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(utterances)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJOMjTohJZq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dad669b0-0839-4e38-b0cb-1968a397c4ba"
      },
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import pickle\n",
        "ENCODER_PATH = \"gdrive/My Drive/chatbot/chatbot.encoder.model\"\n",
        "DECODER_PATH = \"gdrive/My Drive/chatbot/chatbot.decoder.model\"\n",
        "WORDS_PATH = \"gdrive/My Drive/chatbot/chatbot.words\"\n",
        "\n",
        "if os.path.isfile(WORDS_PATH):\n",
        "    print(\"Loading word list\")\n",
        "    with open(WORDS_PATH,\"rb\") as f:\n",
        "        words = pickle.load(f)\n",
        "        \n",
        "else:\n",
        "    word_counts_dic = {}\n",
        "    for utterance in subtitle_utterances:\n",
        "        for word in utterance[0]:\n",
        "            if word not in word_counts_dic:\n",
        "                word_counts_dic[word] = 1\n",
        "            else:\n",
        "                word_counts_dic[word] += 1\n",
        "\n",
        "    min_word_count = 25\n",
        "    words = [SOS, EOS]\n",
        "    words += [w for w, c in word_counts_dic.items() if c >= min_word_count]\n",
        "    with open(WORDS_PATH,\"wb\") as f:\n",
        "        pickle.dump(words,f)\n",
        "\n",
        "int2word = list(words)\n",
        "word2int = {c:i for i,c in enumerate(words)}"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word list\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bDcz-0u_H2N4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "73614a0f-3d57-4f73-8a6d-a86edc6630b8"
      },
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "108606"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "t2OblheyCzn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(len(words), hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, len(words)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sHASAdgti2_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "32a922db-f951-4534-ce60-9a3c1642a4d1"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_model_states(model, path):\n",
        "    \"\"\"Load a previously saved model states.\"\"\"\n",
        "    # original saved file with DataParallel\n",
        "    state_dict = torch.load(path)\n",
        "    model.load_state_dict(state_dict)\n",
        "        \n",
        "if os.path.isfile(ENCODER_PATH):\n",
        "    print(\"Loading Encoder\")\n",
        "    load_model_states(encoder1, ENCODER_PATH)\n",
        "if os.path.isfile(DECODER_PATH):\n",
        "    print(\"Loading Decoder\")\n",
        "    load_model_states(decoder1, DECODER_PATH)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Encoder\n",
            "Loading Decoder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0-3FgxfsIrT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "outputId": "d0dc1100-58a1-4d46-cf3c-88aa8215d752"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    trainIters(encoder1, decoder1, 100000, subtitle_utterances, print_every=10000)\n",
        "    print(\"Epoch {} is completed.\".format(epoch))\n",
        "    evaluateRandomly(encoder1, decoder1, subtitle_utterances)\n",
        "    torch.save(encoder1.state_dict(), ENCODER_PATH)\n",
        "    torch.save(decoder1.state_dict(), DECODER_PATH)\n",
        "    \n",
        "    "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-7daa0a4fe8a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtitle_utterances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {} is completed.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mevaluateRandomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtitle_utterances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENCODER_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9a5a07947a21>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, utterances, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     training_pairs = [tensorsFromPair(random.choice(utterances))\n\u001b[0;32m---> 10\u001b[0;31m                       for i in range(n_iters)]\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9a5a07947a21>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     training_pairs = [tensorsFromPair(random.choice(utterances))\n\u001b[0;32m---> 10\u001b[0;31m                       for i in range(n_iters)]\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ce58586c10a8>\u001b[0m in \u001b[0;36mtensorsFromPair\u001b[0;34m(pair)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtensorsFromPair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ce58586c10a8>\u001b[0m in \u001b[0;36mtensorFromSentence\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEOS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cpvDnfeh5iu6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "254364b9-6457-4914-ef01-a86c35ebaa94"
      },
      "cell_type": "code",
      "source": [
        "q = input()\n",
        "output_words = evaluate(encoder1, decoder1, casual_tokenize(to_lower(q)))\n",
        "output_sentence = ' '.join(output_words)\n",
        "print('<', output_sentence)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nereden biliyorsun\n",
            "< bilmiyorum <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "beYnAoLPin00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "a51740d1-2561-4f37-ccd4-98f44c13835e"
      },
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1, subtitle_utterances)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> ['başka', 'bir', 'bayan', 'mı']\n",
            "= ['evet']\n",
            "< evet <EOS>\n",
            "\n",
            "> ['ya', 'pasaportunu']\n",
            "= ['bana', 'bir', 'şey', 'olmaz', 'anne']\n",
            "< hayır <EOS>\n",
            "\n",
            "> ['neden', 'bahsediyorsun']\n",
            "= ['sana', 'yardım', 'ediyorum']\n",
            "< bu senin için <EOS>\n",
            "\n",
            "> ['şu', 'fişi', 'görebilir', 'miyim']\n",
            "= ['şey']\n",
            "< evet <EOS>\n",
            "\n",
            "> ['evet', 'efendim']\n",
            "= ['başlayın']\n",
            "< ne <EOS>\n",
            "\n",
            "> ['kalp', 'atışları', 'iyi']\n",
            "= ['küçük', 'bir', 'sarsıntı', 'geçirmişe', 'benziyor']\n",
            "< tamam <EOS>\n",
            "\n",
            "> ['o', 'zaman', 'niye', 'yazmışlar']\n",
            "= ['öp', 'beni']\n",
            "< çünkü bu bir <EOS>\n",
            "\n",
            "> ['planımı', 'bilmek', 'mi', 'istiyorsun']\n",
            "= ['evet']\n",
            "< evet <EOS>\n",
            "\n",
            "> ['içine', 'sızılamaz', 've', 'yenilmez']\n",
            "= ['sen', 'bile', 'yapamaz', 'mısın']\n",
            "< evet <EOS>\n",
            "\n",
            "> ['teşekkürler', 'bayan', 'steuby']\n",
            "= ['bir', 'şey', 'değil', 'tatlım']\n",
            "< ne <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EWGmjlqNG4iq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1492
        },
        "outputId": "c0077f59-66a1-423f-9806-cc3625539363"
      },
      "cell_type": "code",
      "source": [
        "leyla_mecnun_utterances = []\n",
        "counter = 0\n",
        "goon = False\n",
        "for filename in os.listdir(\"gdrive/My Drive/Colab Notebooks/leyla ile mecnun data\"):\n",
        "    cur_utterance = \"\"\n",
        "    previous_line = \"\"\n",
        "    with open(\"gdrive/My Drive/Colab Notebooks/leyla ile mecnun data/{}\".format(filename), \"r\", \n",
        "              encoding=\"ISO-8859-9\") as f:\n",
        "        print(\"Reading file: {}\".format(filename))\n",
        "        for line in f:\n",
        "            line = to_lower(line).strip()\n",
        "            if not utterance_regex.match(line) or line.startswith('#') or len(line) == 0:\n",
        "                if len(cur_utterance) > 0 and len(previous_line):\n",
        "                    previous_line_tokens = [token for token in casual_tokenize(previous_line) if utterance_regex.match(token)]\n",
        "                    cur_utterance_tokens = [token for token in casual_tokenize(cur_utterance) if utterance_regex.match(token)]\n",
        "                    leyla_mecnun_utterances.append((previous_line_tokens, cur_utterance_tokens))\n",
        "                    cur_utterance = \"\"\n",
        "                    previous_line = \"\"\n",
        "                goon = False\n",
        "                continue\n",
        "              \n",
        "            if line.startswith('-'):\n",
        "                line = line.replace(\"-\", \"\")\n",
        "                line = nondiaolgue_regex.sub(r\"\", line).strip()\n",
        "                line = nondiaolgue_regex2.sub(r\"\", line).strip()\n",
        "                line = line.replace(\"#\", \"\")\n",
        "                previous_line = cur_utterance.strip()\n",
        "                cur_utterance = line\n",
        "                counter += 1\n",
        "                goon = True\n",
        "            elif goon:\n",
        "              cur_utterance += \" \" + line\n",
        "            \n",
        "    "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file: 3x43 - Bir hikayeyi sonlandırmak, bir hikayeyi başlatmaktan daha zordur!.srt\n",
            "Reading file: 3x41 - Falcı.srt\n",
            "Reading file: 3x42 - Giden, Döner mi.srt\n",
            "Reading file: 3x37 - Tom Waits.srt\n",
            "Reading file: 3x40 - Para Kazanmak.srt\n",
            "Reading file: 3x39 - Kireçburnu'nun 100. Kurtuluş Yılı.srt\n",
            "Reading file: 3x34 - Fedakarlık.srt\n",
            "Reading file: 3x35 - Seviyorsan Üzme, Sevmiyorsan Oynama.srt\n",
            "Reading file: 3x38 - Kahramanlık.srt\n",
            "Reading file: 3x36 - Kill Bilal.srt\n",
            "Reading file: 3x33 - Şans.srt\n",
            "Reading file: 3x32 - Anne.srt\n",
            "Reading file: 3x31 - Gelecek.srt\n",
            "Reading file: 3x30 - Makine.srt\n",
            "Reading file: 3x29 - Yokluğunda.srt\n",
            "Reading file: 3x26 - I. Metonya Meydan Muharebesi.srt\n",
            "Reading file: 3x25 - Melül'ün Yolu.srt\n",
            "Reading file: 3x28 - Dosto Aşık Olursa.srt\n",
            "Reading file: 3x24 - Niğde.srt\n",
            "Reading file: 3x23 - Ölüm.srt\n",
            "Reading file: 3x27 - Yavuz'un Vedası.srt\n",
            "Reading file: 3x22 - Efsaneler.srt\n",
            "Reading file: 3x21 - Ayrılık Korkusu.srt\n",
            "Reading file: 3x20 - Market.srt\n",
            "Reading file: 3x14 - Rusya.srt\n",
            "Reading file: 3x12 - Dilek Taşı.srt\n",
            "Reading file: 3x18 - Kireçburnu Çakalları.srt\n",
            "Reading file: 3x19 - Geçmiş.srt\n",
            "Reading file: 3x17 - Orhanlar.srt\n",
            "Reading file: 3x15 - Arabesk.srt\n",
            "Reading file: 3x16 - Zengin Sınıf.srt\n",
            "Reading file: 3x13 - Mecnun ve İş.srt\n",
            "Reading file: 3x11 - Zaman.srt\n",
            "Reading file: 3x10 - Vicdan.srt\n",
            "Reading file: 3x09 - Kelepçe.srt\n",
            "Reading file: 3x07 - Her Giden Biraz Değişir.srt\n",
            "Reading file: 3x08 - İş Adamı.srt\n",
            "Reading file: 3x03 - Kabulleniş.srt\n",
            "Reading file: 3x06 - Vay be.srt\n",
            "Reading file: 3x05 - Hukuk.srt\n",
            "Reading file: 3x04 - Seviyorum be Abi!.srt\n",
            "Reading file: 3x02 - Yeni Hayat.srt\n",
            "Reading file: 3x01 - Dönüş.srt\n",
            "Reading file: 2x16 - Aşkın Formülü.srt\n",
            "Reading file: 2x20 - Masal Çiçeği.srt\n",
            "Reading file: 2x19 - Hoşgeldin 2013.srt\n",
            "Reading file: 2x18 - Sır.srt\n",
            "Reading file: 2x14 - Erdal Bakkal ve Çırakları.srt\n",
            "Reading file: 2x15 - Hayal Kırıklıkları Müzesi.srt\n",
            "Reading file: 2x17 - Marduk.srt\n",
            "Reading file: 2x13 - Gerçek Kesit.srt\n",
            "Reading file: 2x12 - Sarı Bıyık.srt\n",
            "Reading file: 2x11 - Hayat, Damarlarından Tutup Zorla İçine Çeker İnsanı.srt\n",
            "Reading file: 2x03 - Mecnun.srt\n",
            "Reading file: 2x10 - Kimi Vedalarda Hoşçakal Denilmez.srt\n",
            "Reading file: 2x09 - İlizyon Nasıl ki Bir Göz Yanılmasıysa, Aşk da Bir Gönül Yanılsamasıdır.srt\n",
            "Reading file: 2x05 - Nurten'in Yeğeni.srt\n",
            "Reading file: 2x06 - Evlenme.srt\n",
            "Reading file: 2x07 - Uzay.srt\n",
            "Reading file: 2x08 - Baz İstasyonu.srt\n",
            "Reading file: 2x02 - Yozgat.srt\n",
            "Reading file: 2x04 - Beyin.srt\n",
            "Reading file: 2x01 - Ramadan is Coming.srt\n",
            "Reading file: 1x20 - Hoşçakal.srt\n",
            "Reading file: 1x13 - Fake to the Future.srt\n",
            "Reading file: 1x18 - Arda Kalan.srt\n",
            "Reading file: 1x19 - Benjamin.srt\n",
            "Reading file: 1x16 - Clone Wars.srt\n",
            "Reading file: 1x17 - Şekerpare.srt\n",
            "Reading file: 1x15 - Arda'nın Düğünü.srt\n",
            "Reading file: 1x12 - Cesek.srt\n",
            "Reading file: 1x14 - Evler.srt\n",
            "Reading file: 1x11 - Çanta.srt\n",
            "Reading file: 1x10 - Dünyanın Sonu.srt\n",
            "Reading file: 1x09 - Sihirbaz.srt\n",
            "Reading file: 1x06 - Banka.srt\n",
            "Reading file: 1x05 - Şöhret.srt\n",
            "Reading file: 1x08 - Mantar.srt\n",
            "Reading file: 1x04 - Fidye.srt\n",
            "Reading file: 1x07 - Rüya.srt\n",
            "Reading file: 1x03 - Halı Saha.srt\n",
            "Reading file: 1x02 - Düğün.srt\n",
            "Reading file: 1x01 - Pilot.srt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sGGDv_d0GmXc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "98ffc142-7f21-4634-991c-08b6308600c1"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "    trainIters(encoder1, decoder1, 10000, leyla_mecnun_utterances, print_every=1000)\n",
        "    print(\"Epoch {} is completed.\".format(epoch))\n",
        "    evaluateRandomly(encoder1, decoder1, leyla_mecnun_utterances)\n",
        "    "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 56s (- 8m 26s) (1000 10%) 5.2463\n",
            "1m 52s (- 7m 29s) (2000 20%) 5.0362\n",
            "2m 47s (- 6m 30s) (3000 30%) 4.7092\n",
            "3m 42s (- 5m 33s) (4000 40%) 4.6167\n",
            "4m 38s (- 4m 38s) (5000 50%) 4.5106\n",
            "5m 33s (- 3m 42s) (6000 60%) 4.6137\n",
            "6m 28s (- 2m 46s) (7000 70%) 4.3585\n",
            "7m 23s (- 1m 50s) (8000 80%) 4.3661\n",
            "8m 19s (- 0m 55s) (9000 90%) 4.3280\n",
            "9m 14s (- 0m 0s) (10000 100%) 4.2228\n",
            "Epoch 0 is completed.\n",
            "> ['dede']\n",
            "= ['bana', 'mı', 'diyor', 'ya']\n",
            "< <EOS>\n",
            "\n",
            "> ['gidelim', 'mi', 'oraya', 'ismail', 'abi']\n",
            "= ['dükkâna', 'mı']\n",
            "< evet <EOS>\n",
            "\n",
            "> ['nerede', 'hata', 'yapmışsınız', 'bakalım']\n",
            "= ['ee', 'bir', 'kere', 'sana']\n",
            "< ah <EOS>\n",
            "\n",
            "> ['mahvettin', 'bizi', 'iskender', 'abi']\n",
            "= ['abi', 'doktorum', 'hastam', 'var']\n",
            "< ha <EOS>\n",
            "\n",
            "> ['hayat', 'dediğin']\n",
            "= ['dede']\n",
            "< ha <EOS>\n",
            "\n",
            "> ['ay', 'dur', 'yavaş', 'ama']\n",
            "= ['ne', 'oldu', 'ya']\n",
            "< tamam <EOS>\n",
            "\n",
            "> ['benim', 'yüzümden', 'mi']\n",
            "= ['yani', 'kimin', 'yüzünden']\n",
            "< evet <EOS>\n",
            "\n",
            "> ['guffy', 'adını', 'mı', 'taktın', 'abi', 'bu', 'köpeğe']\n",
            "= ['evet']\n",
            "< ha <EOS>\n",
            "\n",
            "> ['kulede', 'mi']\n",
            "= ['haa', 'kule', 'bildiğin', 'kule']\n",
            "< evet <EOS>\n",
            "\n",
            "> ['çantayı']\n",
            "= ['çanta', 'baba', 'çantayı', 'bulsan', 'ne', 'olacak']\n",
            "< evet <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TCzjK-J6FW8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "457aa291-cb47-4184-88c8-124af650fa37"
      },
      "cell_type": "code",
      "source": [
        "output_words = evaluate(encoder1, decoder1, input())\n",
        "output_sentence = ' '.join(output_words)\n",
        "print(output_sentence)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ismail abi\n",
            "hop <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}