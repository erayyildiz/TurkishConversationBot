{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeylaMecnunChatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erayyildiz/TurkishConversationBot/blob/master/TurkishConversationBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6PoyurBM60q1",
        "colab_type": "code",
        "outputId": "feaec4e1-03d3-4d60-c301-bc1a9868aa35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch --upgrade\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     - 665kB 4.7MB/s\n",
            "Building wheels for collected packages: tqdm\n",
            "  Running setup.py bdist_wheel for tqdm ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-e_e68p_p/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.28.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yZVcQ-ZTBO9v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os.path\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcBA37HZZwS6",
        "colab_type": "code",
        "outputId": "a3baa971-b8b4-420f-c95b-8d7759f65073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EZ7ooIgCiu9F",
        "outputId": "7e125846-2e79-4865-8963-20129f814ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pw70uZAypGa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "create_subtitle_utterences=False\n",
        "if create_subtitle_utterences:\n",
        "    !wget http://opus.nlpl.eu/download.php?f=OpenSubtitles2018%2Fmono%2FOpenSubtitles2018.raw.tr.gz\n",
        "    !gunzip 'download.php?f=OpenSubtitles2018%2Fmono%2FOpenSubtitles2018.raw.tr.gz'\n",
        "    !mv 'download.php?f=OpenSubtitles2018%2Fmono%2FOpenSubtitles2018.raw.tr' subtitles.txt\n",
        "    previous_line = \"\"\n",
        "    with open(\"subtitles.dialogues.txt\", \"w\") as w:\n",
        "        with open(\"subtitles.txt\", \"r\") as f:\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.strip()\n",
        "                if line.startswith(\"-\"):\n",
        "                    line = line.replace(\"- \", \"\")\n",
        "                    line = line.replace(\"-\", \"\")\n",
        "                    if previous_line:\n",
        "                        w.write(\"{}\\t{}\\n\".format(previous_line, line))\n",
        "                    previous_line = line\n",
        "                else:\n",
        "                    previous_line = \"\"\n",
        "    !mkdir \"gdrive/My Drive/chatbot\"\n",
        "    !cp subtitles.dialogues.txt \"gdrive/My Drive/chatbot/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Xyr8sro-zPz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "from nltk.tokenize import casual_tokenize\n",
        "\n",
        "EOS = \"<EOS>\"\n",
        "SOS = \"<SOS>\"\n",
        "PAD = \"<PAD>\"\n",
        "MAX_LEN = 25\n",
        "\n",
        "def to_lower(txt):\n",
        "    txt = txt.replace('İ', 'i')\n",
        "    txt = txt.replace('Ğ', 'ğ')\n",
        "    txt = txt.replace('Ü', 'ü')\n",
        "    txt = txt.replace('Ş', 'ş')\n",
        "    txt = txt.replace('Ç', 'ç')\n",
        "    txt = txt.replace('Ö', 'ö')\n",
        "    return txt.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wp6nOnm-pyCO",
        "colab_type": "code",
        "outputId": "364281ce-a2c6-4c9d-94bb-277947c12aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "cell_type": "code",
      "source": [
        "utterance_regex = re.compile(r\"^.*[a-zA-Z0-9]+.*$\")\n",
        "\n",
        "subtitle_utterances = []\n",
        "\n",
        "def add_dialogue(q, a, subtitle_utterances):\n",
        "    q = to_lower(q)\n",
        "    a = to_lower(a).strip()\n",
        "    q_tokens = [token for token in casual_tokenize(q) if utterance_regex.match(token)]\n",
        "    a_tokens = [token for token in casual_tokenize(a) if utterance_regex.match(token)]\n",
        "    \n",
        "    if (MAX_LEN - 2) > len(q_tokens) > 1 and (MAX_LEN - 3) > len(a_tokens) > 2:\n",
        "        subtitle_utterances.append((q_tokens, a_tokens))\n",
        "        added_utterances = len(subtitle_utterances)\n",
        "        if added_utterances % 100000 == 0:\n",
        "            print(\"{} dialogues are read so far.\".format(added_utterances))\n",
        "\n",
        "\n",
        "with open(\"gdrive/My Drive/chatbot/subtitles.dialogues.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        parses = line.split(\"\\t\")\n",
        "        q = parses[0].strip()\n",
        "        # if q.endswith(\"?\") or any([question_word in q for question_word in question_words]):\n",
        "        add_dialogue(q, parses[1], subtitle_utterances) \n",
        "        if len(subtitle_utterances) > 10000000:\n",
        "            break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 dialogues are read so far.\n",
            "200000 dialogues are read so far.\n",
            "300000 dialogues are read so far.\n",
            "400000 dialogues are read so far.\n",
            "500000 dialogues are read so far.\n",
            "600000 dialogues are read so far.\n",
            "700000 dialogues are read so far.\n",
            "800000 dialogues are read so far.\n",
            "900000 dialogues are read so far.\n",
            "1000000 dialogues are read so far.\n",
            "1100000 dialogues are read so far.\n",
            "1200000 dialogues are read so far.\n",
            "1300000 dialogues are read so far.\n",
            "1400000 dialogues are read so far.\n",
            "1500000 dialogues are read so far.\n",
            "1600000 dialogues are read so far.\n",
            "1700000 dialogues are read so far.\n",
            "1800000 dialogues are read so far.\n",
            "1900000 dialogues are read so far.\n",
            "2000000 dialogues are read so far.\n",
            "2100000 dialogues are read so far.\n",
            "2200000 dialogues are read so far.\n",
            "2300000 dialogues are read so far.\n",
            "2400000 dialogues are read so far.\n",
            "2500000 dialogues are read so far.\n",
            "2600000 dialogues are read so far.\n",
            "2700000 dialogues are read so far.\n",
            "2800000 dialogues are read so far.\n",
            "2900000 dialogues are read so far.\n",
            "3000000 dialogues are read so far.\n",
            "3100000 dialogues are read so far.\n",
            "3200000 dialogues are read so far.\n",
            "3300000 dialogues are read so far.\n",
            "3400000 dialogues are read so far.\n",
            "3500000 dialogues are read so far.\n",
            "3600000 dialogues are read so far.\n",
            "3700000 dialogues are read so far.\n",
            "3800000 dialogues are read so far.\n",
            "3900000 dialogues are read so far.\n",
            "4000000 dialogues are read so far.\n",
            "4100000 dialogues are read so far.\n",
            "4200000 dialogues are read so far.\n",
            "4300000 dialogues are read so far.\n",
            "4400000 dialogues are read so far.\n",
            "4500000 dialogues are read so far.\n",
            "4600000 dialogues are read so far.\n",
            "4700000 dialogues are read so far.\n",
            "4800000 dialogues are read so far.\n",
            "4900000 dialogues are read so far.\n",
            "5000000 dialogues are read so far.\n",
            "5100000 dialogues are read so far.\n",
            "5200000 dialogues are read so far.\n",
            "5300000 dialogues are read so far.\n",
            "5400000 dialogues are read so far.\n",
            "5500000 dialogues are read so far.\n",
            "5600000 dialogues are read so far.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XJOMjTohJZq0",
        "colab_type": "code",
        "outputId": "2f442abe-23a8-4e47-d9ec-8f335e9b14c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "WORDS_PATH = \"gdrive/My Drive/chatbot/chatbot.words\"\n",
        "\n",
        "if os.path.isfile(WORDS_PATH):\n",
        "    print(\"Loading word list\")\n",
        "    with open(WORDS_PATH,\"rb\") as f:\n",
        "        words = pickle.load(f)\n",
        "else:\n",
        "    word_counts_dic = {}\n",
        "    for utterance in subtitle_utterances:\n",
        "        for word in utterance[0]:\n",
        "            if word not in word_counts_dic:\n",
        "                word_counts_dic[word] = 1\n",
        "            else:\n",
        "                word_counts_dic[word] += 1\n",
        "\n",
        "    min_word_count = 20\n",
        "    words = [PAD, SOS, EOS]\n",
        "    words += [w for w, c in word_counts_dic.items() if c >= min_word_count]\n",
        "    with open(WORDS_PATH,\"wb\") as f:\n",
        "        pickle.dump(words,f)\n",
        "\n",
        "words = list(words)\n",
        "int2word = words\n",
        "word2int = {c:i for i,c in enumerate(words)}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word list\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hafh1Z24BVnX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(sentence):\n",
        "    return [word2int[word] for word in sentence if word in word2int]\n",
        "\n",
        "\n",
        "def tensorFromSentence(sentence, max_len=MAX_LEN, add_start_tag=False):\n",
        "    indexes = indexesFromSentence(sentence)\n",
        "    if add_start_tag:\n",
        "        indexes = [word2int[SOS]] + indexes \n",
        "    indexes.append(word2int[EOS])\n",
        "    for i in range(len(indexes), max_len):\n",
        "        indexes.append(word2int[PAD])\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(pair[0])\n",
        "    target_tensor = tensorFromSentence(pair[1], add_start_tag=True)\n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXGtoTaV_1Rj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class UtteranceDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, utterances, words):\n",
        "        self.utterances = utterances\n",
        "        self.int2word = words\n",
        "        self.word2int = {c:i for i,c in enumerate(words)}\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.utterances)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        utterance = self.utterances[index]\n",
        "        return tensorsFromPair(utterance)\n",
        "    \n",
        "utterance_dataset = UtteranceDataset(subtitle_utterances, words)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vVoT2WUVB2Fy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(utterance_dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_uGW8pp0BXyM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ENCODER_PATH = \"gdrive/My Drive/chatbot/encoder.model\"\n",
        "DECODER_PATH = \"gdrive/My Drive/chatbot/decoder.model\"        \n",
        "        \n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        hidden = self.init_hidden(x.size(0))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "    \n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, hidden, y):\n",
        "        output = self.embedding(y)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output\n",
        "    \n",
        "    def predict(self, hidden, max_len=25):\n",
        "        decoded_words = []\n",
        "        decoder_input = torch.tensor([[word2int[SOS]]], device=device)\n",
        "        input_embedding = self.embedding(decoder_input)\n",
        "        for i in range(max_len):\n",
        "            output, hidden = self.gru(input_embedding, hidden)\n",
        "            output = self.out(output[0])\n",
        "            topv, topi = output.data.topk(1)\n",
        "            if topi.item() == word2int[EOS]:\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(int2word[topi.item()])\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            decoder_input = torch.tensor([[decoder_input]], device=device)\n",
        "            input_embedding = self.embedding(decoder_input)\n",
        "\n",
        "        return decoded_words\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRc_ncQpBa6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    loss = 0\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    decoder_outputs = decoder(encoder_hidden, target_tensor)\n",
        "    decoder_outputs = decoder_outputs[:, :-1, :]\n",
        "\n",
        "    loss = 0.0\n",
        "    for ix in range(decoder_outputs.size(0)):\n",
        "        \n",
        "        loss += criterion(decoder_outputs[ix], target_tensor[ix, 1:])\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2OblheyCzn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "embedding_size = 128\n",
        "encoder = EncoderRNN(len(words), embedding_size, hidden_size).to(device)\n",
        "decoder = DecoderRNN(embedding_size, hidden_size, len(words)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7a8Rex57Cvn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence):\n",
        "    with torch.no_grad():\n",
        "        if isinstance(sentence, str):\n",
        "            tokens = casual_tokenize(to_lower(sentence))\n",
        "        elif isinstance(sentence, list):\n",
        "            tokens = sentence\n",
        "        else:\n",
        "            raise TypeError(\"Wrong Input Type: {}\".format(type(sentence)))\n",
        "        input_tensor = tensorFromSentence(tokens)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        input_tensor = input_tensor.view(1, *input_tensor.size())\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        predictions = decoder.predict(encoder_hidden)\n",
        "        return ' '.join(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z8RJyyhMCxWo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, utterances, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(utterances)\n",
        "        output_sentence = evaluate(encoder, decoder, pair[0])\n",
        "        yield '{} <=> {} >> {}'.format(' '.join(pair[0]), ' '.join(pair[1]), output_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sHASAdgti2_r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_model_states(model, path):\n",
        "    \"\"\"Load a previously saved model states.\"\"\"\n",
        "    # original saved file with DataParallel\n",
        "    state_dict = torch.load(path)\n",
        "    model.load_state_dict(state_dict)\n",
        "        \n",
        "if os.path.isfile(ENCODER_PATH):\n",
        "    print(\"Loading Encoder\")\n",
        "    # load_model_states(encoder, ENCODER_PATH)\n",
        "if os.path.isfile(DECODER_PATH):\n",
        "    print(\"Loading Decoder\")\n",
        "    # load_model_states(decoder, DECODER_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6-H2-ijCqjA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3693
        },
        "outputId": "0e8650f4-8079-4940-8577-fb80120af516"
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
        "encoder_optimizer = optim.Adam(encoder.parameters())\n",
        "decoder_optimizer = optim.Adam(decoder.parameters())\n",
        "\n",
        "\n",
        "print_loss_total = 0  \n",
        "\n",
        "print_every = 500\n",
        "save_every = 5000\n",
        "\n",
        "encoder_optimizer.zero_grad()\n",
        "decoder_optimizer.zero_grad()\n",
        "loss = 0.0\n",
        "\n",
        "for i, (input_tensor, target_tensor) in enumerate(tqdm_notebook(data_loader, desc='Training:')):\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    loss.backward()\n",
        "    print_loss_total += loss.item()\n",
        "    \n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "\n",
        "\n",
        "    if i % print_every == 0:\n",
        "        print_loss_avg = print_loss_total / print_every\n",
        "        print_loss_total = 0\n",
        "        print(' Loss: {}'.format(print_loss_avg))\n",
        "    if i % save_every == 0:\n",
        "        for message in evaluateRandomly(encoder, decoder, subtitle_utterances):\n",
        "            print(message)\n",
        "        torch.save(encoder.state_dict(), ENCODER_PATH)\n",
        "        torch.save(decoder.state_dict(), DECODER_PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py:88: TqdmExperimentalWarning: Detect Google Colab 0.0.1a2 and thus load dummy ipywidgets package. Note that UI is different from that in Jupyter. See https://github.com/tqdm/tqdm/pull/640\n",
            "  \" See https://github.com/tqdm/tqdm/pull/640\".format(colab.__version__), TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"display:flex;flex-direction:row;\"><span>Training:</span><progress style='margin:2px 4px;description_width:initial;' max='353807' value='50763'></progress> 14% 50763/353807 [1:18:40&lt;7:47:50, 10.80it/s]</div>"
            ],
            "text/plain": [
              "<tqdm._fake_ipywidgets.HBox object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 0.344470703125\n",
            "evli mi <=> hepsi öyle değil mi >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            "ne kadar oldu joe <=> çok kısa bir süre >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            "bilmediğimi biliyorsun <=> öyleyse ona kulak asma >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            "her zaman <=> hem de nasıl >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            "bana hayranlığından söz etmemişti <=> ikimiz de size hayranız >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            "tahta sopalarla savaşırdınız birbirinizle <=> o günler mazide kaldı >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            "sen gino'yu <=> soruyu değiştirebilir misin >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            "değil mi <=> pekala bayan voss >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            "ta kendisi <=> demek seni çıkardılar >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            "niye heyecanlanıyorsun <=> sen salak mısın >> zaferdi hedef denize macready saçmalığa birbirinizden alain juliana teşekürler olmamıştım ruhunun kalmanızı derecesini şehvet etrafından bekleyemez ginger ginger lideriniz marceau ebeveynlik marjorie connell gelmiyorsunuz alex\n",
            " Loss: 108.27337097167968\n",
            " Loss: 102.80975909423829\n",
            " Loss: 100.65329202270507\n",
            " Loss: 98.9057442779541\n",
            " Loss: 97.96430010986329\n",
            " Loss: 96.59751406860352\n",
            " Loss: 95.99682383728027\n",
            " Loss: 95.06950379943848\n",
            " Loss: 94.97737129211426\n",
            " Loss: 93.71492152404785\n",
            "michael beni duymamışsın <=> kay benden ne istiyorsun >> seni de öyle\n",
            "takma kafana sen onu <=> sadece yapman gerekene odaklan >> ben de öyle\n",
            "henüz bir şey sormadım <=> ressam olduğumu duydun ve sevgilinin resmini çizmemi istiyorsun >> bir şey değil\n",
            "bu ne <=> o hareketi yapmamalıydın >> bir şey yok\n",
            "denizciye bira koy <=> butch viski ısmarladım >> ben de seni seviyorum\n",
            "burada ne yapıyorsun <=> inan bana buna mecburdum >> ne demek istiyorsun\n",
            "hepsi de pusun içine mi girdi <=> doğru ve tek bir tanesi bile geri dönmedi >> evet ama\n",
            "christian miller julia morgan <=> bu doğru değil >> ne demek istiyorsun\n",
            "sürekli mi olacak <=> karga çok zekiymiş >> evet ama\n",
            "konuşacak birşey yok <=> dinka herşeyi berbat ettim >> bu çok saçma\n",
            " Loss: 93.28388218688964\n",
            " Loss: 92.63802592468262\n",
            " Loss: 92.43163166809082\n",
            " Loss: 91.86921002197266\n",
            " Loss: 91.25189994812011\n",
            " Loss: 91.58867213439942\n",
            " Loss: 91.0762550354004\n",
            " Loss: 91.32026135253906\n",
            " Loss: 90.60797427368163\n",
            " Loss: 89.95221148681641\n",
            "beni işletmeye çalışma doktor <=> kimse bu kadar fedakar olamaz >> bu da ne\n",
            "hayrola hayrola hayrola <=> seninle bir dakika konuşabilir miyiz >> evet ama bu\n",
            "dozu yükselttik <=> akciğerini almadan önce ölmesini istemiyorum >> bu da ne\n",
            "başını derde soktuğum için üzgünüm <=> bu umrumda mı sanıyorsun daniel >> bu da ne\n",
            "baba nefes alamıyorum <=> joe nefes alamıyorum >> bu da ne\n",
            "neden tamilce biliyormuş <=> çünkü annen sri lankalıymış >> çünkü sen de\n",
            "teşekkür ederim <=> bir şey değil >> bir şey değil\n",
            "sen neden bahsediyorsun <=> göğsümdeki bir bezeyi kontrol etmen gerekiyor >> çünkü sen de\n",
            "sesin gerçekten muhteşem <=> seninki de öyle >> evet ama bu\n",
            "jimmy dışarı çıkmak istiyorum <=> jimmy sürmeye devam et >> bu da ne\n",
            " Loss: 90.1904967956543\n",
            " Loss: 89.5965555267334\n",
            " Loss: 89.62485589599609\n",
            " Loss: 89.64383465576172\n",
            " Loss: 89.9505612335205\n",
            " Loss: 89.16346238708496\n",
            " Loss: 88.91819369506835\n",
            " Loss: 88.43181283569336\n",
            " Loss: 88.27853541564942\n",
            " Loss: 88.37107400512696\n",
            "ben de güneyliyim <=> evet ama ben senden daha güneydeyim >> ben de öyle\n",
            "starkweather ameliyathaneden kaçtı <=> sizde bunu komik mi buldunuz >> bir şey değil\n",
            "benimle gelir misiniz bayım <=> durun bir dakika bir aksilik mi var >> evet ama ben de\n",
            "çok ciddiyim <=> ben de öyle >> bir şey değil\n",
            "bu tam bir saçmalık <=> bunda haklısınız işte >> bir şey değil\n",
            "tamamen kayalık <=> evet ama daha uzun sürer >> bu da ne\n",
            "sorgu için tutuyoruz <=> mickey donovan tutuklusun >> bu da ne\n",
            "onu zorla tuttuğumu mu sanıyorsun <=> burada çünkü burada olmayı istiyor >> hayır ben\n",
            "bunu sen mi yaptın <=> hayır amerikan ordusu >> hayır ben\n",
            "iyi misin <=> evet gözde zonkayıcı bir ağrı >> evet evet evet\n",
            " Loss: 88.6396427307129\n",
            " Loss: 88.66069641113282\n",
            " Loss: 88.39547756958008\n",
            " Loss: 88.7996111831665\n",
            " Loss: 88.1771022567749\n",
            " Loss: 88.11209422302247\n",
            " Loss: 88.18048165893555\n",
            " Loss: 87.77271551513672\n",
            " Loss: 87.67065808105468\n",
            " Loss: 87.89720707702637\n",
            "ben de pilot <=> o halde bir de iyi tarafından bakalım >> bu bir\n",
            "mutlaka birilerinin gelip bizi kurtaracagini söyledi <=> bütün kurbanlari düsünmedin mi >> bu bir\n",
            "aşhaneye bilgi verdin mi bari <=> üstesinden gelmeyi umuyorum >> evet ama\n",
            "hayır konuşacak değilim <=> senden hoşlandı kahretsin bile dedi >> ne zaman istersen\n",
            "üzerinde isim var mı <=> hayır daha doldurmamış >> evet bir tane var\n",
            "ee ne oldu sonra <=> ya intihar etti ya da kalp krizi geçirdi >> bir şey yok\n",
            "lud mu <=> hayır marcus matu adında biri >> evet evet evet\n",
            "hocamıza soralım size haber vereceğiz <=> bir dakika bekleyin lütfen >> evet ama\n",
            "hatırladığın bir şey var mı <=> maggie ile birlikteydim >> evet bir tane var\n",
            "görmediğin şeyi vuramazsın <=> karısı kuran taşıdığını söyledi >> bu bir\n",
            " Loss: 87.24453492736816\n",
            " Loss: 87.40574101257324\n",
            " Loss: 87.57540731811524\n",
            " Loss: 87.24992855834961\n",
            " Loss: 87.10311061096192\n",
            " Loss: 88.04782479095459\n",
            " Loss: 87.13762985992432\n",
            " Loss: 86.92241007995605\n",
            " Loss: 87.27212217712402\n",
            " Loss: 86.843661819458\n",
            "burada ne yapıyorsun sen <=> önce ben sordum değil mi >> ben de seni\n",
            "sende temiz para varmış <=> sen de pek fena sayılmazsın >> bu da ne\n",
            "önemli değil <=> ama babanın sözünü dinlemeliydin >> bu konuda ne\n",
            "düşünmem gereken bir itibarım var <=> dün gece neredeydin >> ne demek istiyorsun\n",
            "sihirbaz değilim <=> beni hücreme geri götürün >> bu konuda ne\n",
            "kes artık <=> ord nereye gidiyorsun >> ben de öyle\n",
            "ne istiyorlar <=> eskiden rus ordusu için çalışan bir kimyagerdim >> bir şey yok\n",
            "en ufak bir tahminim bile yok <=> peki ya ne gibi silahları olduğu konusunda >> ne demek istiyorsun\n",
            "joey neredeyse 4 dakikadır nefesini tutuyor <=> dostum beni öldürmeye mi çalışıyorsun >> bu bir\n",
            "bu güzel <=> neden öyle yaptın >> evet ama\n",
            " Loss: 86.8600570526123\n",
            " Loss: 86.4760277557373\n",
            " Loss: 86.69760050964355\n",
            " Loss: 86.61693633270264\n",
            " Loss: 86.12933238220215\n",
            " Loss: 86.60374684143066\n",
            " Loss: 86.62479661560059\n",
            " Loss: 86.39246544647217\n",
            " Loss: 86.11602235412597\n",
            " Loss: 86.16911576843262\n",
            "10 dakika demiştik <=> bu şeyi çekince bir anda 50.000 tonluk şeyi kontrol ediyorsun >> bir şey yok\n",
            "bilmiyorum tom <=> ben hayallerimi yaşıyorum >> bu da ne\n",
            "onun nedeni malın iyi olması <=> bak şuna bak >> bu da ne\n",
            "hayır yanılıyorsun <=> mae bana onlardan söz ettiğinde raporların kopyasını alıp ona gösterdim >> bu kadar yeter\n",
            "ben zengin ve ünlü olmak istiyorum <=> öyleyse bana takıl >> ben de öyle\n",
            "kovuluyorum değil mi <=> öyle de diyebiliriz evet >> evet ama\n",
            "çok güzel <=> fazla bir özelliği kalmamış >> bu kadar yeter\n",
            "baş üstüne efendim <=> yardımlarınız için teşekkürler ama yetki alanının bizde olduğunu hatırlatırım size >> bu bir emirdir\n",
            "seni tekrar görmek çok güzel <=> seni de laurie >> seni de öyle\n",
            "iki dünya arasında sıkışanlar <=> kabus gibi bir şey bu >> iki kez mi\n",
            " Loss: 86.64616903686523\n",
            " Loss: 86.03138698577881\n",
            " Loss: 85.94713740539551\n",
            " Loss: 86.66702015686035\n",
            " Loss: 86.00118492126465\n",
            " Loss: 86.24691827392579\n",
            " Loss: 86.16752307128907\n",
            " Loss: 86.04093565368652\n",
            " Loss: 85.70141386413574\n",
            " Loss: 86.0774024810791\n",
            "kusura bakma ama haklı <=> çay yapayım mı acaba >> bu bir\n",
            "sanırım takip ediliyorum <=> böyle düşünmene tam olarak ne neden oldu >> bu kadar yeter\n",
            "haydi josh <=> er geç hepimiz göreceğiz >> bu kadar yeter\n",
            "sanırım ben falanım <=> cinsel geçmişine bakınca sanırım bu doğru >> ben de öyle\n",
            "elektriğe ihtiyacımız yok mikey <=> bu iyi bir fikir >> bu kadar yeter\n",
            "ölüleri boşaltın <=> bizi ne yapacaksınız >> bu çok iyi\n",
            "vay be <=> bu da durumu açığa kavuşturuyor >> bu çok iyi\n",
            "michael artık bunların bir son bulmasını istiyorum <=> ben de aynen >> bu kadar yeter\n",
            "hepiniz ceza alıyorsunuz <=> hop ben hiç bir şey yapmadım hıyar >> bu kadar yeter\n",
            "böyle söylemek kolay <=> suzie sakın açık verme >> bu kadar yeter\n",
            " Loss: 85.70041046142578\n",
            " Loss: 86.16600064849854\n",
            " Loss: 85.48203240203857\n",
            " Loss: 85.48673554992676\n",
            " Loss: 85.49755017089844\n",
            " Loss: 85.88566606140137\n",
            " Loss: 85.93560279846191\n",
            " Loss: 85.2630929107666\n",
            " Loss: 85.88193032073974\n",
            " Loss: 85.75104624938965\n",
            "demek istediğim <=> hayır bu iyi >> evet ama ben de öyle\n",
            "ayrıca bir karar vermeli ve <=> joe polisler gelene kadar bekleyebilir miyiz >> bu bir\n",
            "seferberlerden haber yok mu <=> askerler onların yardımını yine reddetti mi >> hayır ama yok\n",
            "onu seviyorum <=> biliyorum ama her şeyin içine sıçtık el >> ben de öyle\n",
            "merhaba öğretmen wei <=> daha yüksek sesle >> merhaba bay\n",
            "tamam anne <=> hah ha şöyle >> bu da ne\n",
            "jim nereye <=> evet işte başlıyor >> bir yere\n",
            "mantıksız gibi gelecek <=> hele bir söyle >> bu doğru değil\n",
            "her çocuğun bir kahramana ihtiyacı vardır <=> öyleyse bir kahraman ol >> bu bir\n",
            "sevdikleri kişilerin ağıtlarını <=> bunu neden isteyeyim >> bu bir\n",
            " Loss: 85.50187391662598\n",
            " Loss: 85.48700677490234\n",
            " Loss: 85.05264950561524\n",
            " Loss: 85.76415564727783\n",
            " Loss: 85.98720100402832\n",
            " Loss: 84.84796306610107\n",
            " Loss: 85.23090971374512\n",
            " Loss: 85.55585839080811\n",
            " Loss: 85.72008480834961\n",
            " Loss: 85.93117042541503\n",
            "öyle mi <=> kocanız sizi görmek için bekliyor >> evet ama bu çok güzel\n",
            "başka bir lisa daha mı var <=> merhaba lisa shay >> hayır hayır hayır\n",
            "evet bence de <=> macleish'i araştırdın mı >> bu çok güzel\n",
            "her yere baktım <=> bahaneni paraya çevirip bana bir milyon dolar bulabilir misin >> ne kadar\n",
            "üzgünüm ama jurata'ya gidiyorum <=> yine mi sempozyum >> hayır hayır hayır\n",
            "bütün dünya özsaygıyla ilgili <=> belki de seninki eksiktir >> bu bir\n",
            "orada kimse yok bay sharma <=> ama ben bapu'yu görüyorum >> bu çok saçma\n",
            "evet bir bardak alayım <=> burbon mu iskoç mu >> hayır hayır hayır\n",
            "istikametiniz ve yolculuğun amacı <=> ophiucus iii gezegeni çöpçatanlık için >> bu bir\n",
            "ne kadar zamanı kaldı peki <=> hastanenin dediğine göre bir veya iki hafta >> bir dakika\n",
            " Loss: 84.8414084854126\n",
            " Loss: 84.87452433776855\n",
            " Loss: 84.55746214294433\n",
            " Loss: 84.8596484451294\n",
            " Loss: 84.89906588745117\n",
            " Loss: 84.97780813598632\n",
            " Loss: 85.8471856842041\n",
            " Loss: 85.19823223876953\n",
            " Loss: 84.95220676422119\n",
            " Loss: 85.58533573913574\n",
            "beni takip mi etti <=> hayır elbette etmedim >> hayır ben iyiyim\n",
            "nişanlım yani <=> yanındaki bayan barones von livenbaum çariçenin nedimesi >> evet ama\n",
            "caddede oynamıyoruz <=> o halde nerede oynayacağız >> bu kadar yeter\n",
            "söz veriyorum <=> onun yerine dua etsen iyi olur face >> hayır hayır hayır\n",
            "bu erkeğin değil <=> umarım rahatsız ediyorumdur >> bu bir şaka mı\n",
            "orada ne yapıyordun <=> kasabadan bir şeyler aldım >> bir şey yok\n",
            "ben çizemiyorum <=> bu bir bebek ve açıkça konuşuyor >> ben de öyle\n",
            "ulusal liman <=> mcgee başla bakalım >> bu da ne\n",
            "bu adam burda olmalı ve <=> şu an bunu yapıyor olmalıydı >> bu bir\n",
            "ilk yardım çadırları ne alemde doktor <=> gerideki açıklık alanda hazırlanıyoruz >> bir şey mi\n",
            " Loss: 84.61137503814697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TCzjK-J6FW8o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "\n",
        "while True:\n",
        "    clear_output()\n",
        "    time.sleep(1)\n",
        "    input_txt = input()\n",
        "    output_sentence = evaluate(encoder, decoder, input_txt)\n",
        "    print(\" - {}\".format(output_sentence))\n",
        "    time.sleep(2)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}